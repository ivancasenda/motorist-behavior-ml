{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import tensorflow as tf \n",
    "import matplotlib.pyplot as plt \n",
    "import pathlib\n",
    "import datetime\n",
    "from math import ceil\n",
    "from src.data.dataset_loader import load_distracted_driver_detection_list, load_dataset_image_label\n",
    "from src.data.image_label_loader import ImageLabelLoader\n",
    "from src.evaluation.metrics import accuracy, confusion_matrix\n",
    "from src.utils.data_util import get_paths, sample_dataset\n",
    "from src.preprocessing.standardizer import Standardizer\n",
    "from src.visualization.history_plotter import plot_loss_acc_history_epoch\n",
    "from src.visualization.weights_visualization import visualize_weights\n",
    "from src.solver.tensorflow_solver import TensorflowSolver\n",
    "\n",
    "# Load tensorboard notebook extenstion \n",
    "%load_ext tensorboard\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 17761\n",
      "Validation set size: 4663\n",
      "Number of classes: 10\n"
     ]
    }
   ],
   "source": [
    "data_dir = pathlib.Path(\"dataset/raw/imgs/train\") # Train directory\n",
    "\n",
    "X_train_filenames, X_val_filenames, y_train_labels, y_val_labels = load_distracted_driver_detection_list(val_size=0.2, split_on_driver=True, random_state=12)\n",
    "train_paths = get_paths(data_dir, X_train_filenames, y_train_labels)\n",
    "val_paths = get_paths(data_dir, X_val_filenames, y_val_labels)\n",
    "class_names = np.array(sorted([item.name for item in data_dir.glob('*')]))\n",
    "\n",
    "num_train = len(train_paths)\n",
    "num_val = len(val_paths)\n",
    "num_classes = len(class_names)\n",
    "\n",
    "print(f'Training set size: {num_train}')\n",
    "print(f'Validation set size: {num_val}')\n",
    "print(f'Number of classes: {num_classes}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CPU = '/cpu:0'\n",
    "GPU = '/device:GPU:0'\n",
    "physical_GPU = tf.config.list_physical_devices('GPU')[0]\n",
    "tf.config.experimental.set_memory_growth(physical_GPU, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = tf.random.Generator.from_seed(123, alg='philox')\n",
    "\n",
    "def wrap_augment(image, label):\n",
    "    seed = rng.make_seeds(2)[0]\n",
    "    image = augment(image, seed)\n",
    "    return image, label\n",
    "\n",
    "def augment(image, seed):\n",
    "    image = tf.image.stateless_random_brightness(image, max_delta=0.5, seed=seed)\n",
    "    image = tf.image.stateless_random_hue(image, max_delta=0.5, seed=seed)\n",
    "    image = tf.image.stateless_random_contrast(image, lower=0.9, upper=1.2, seed=seed)\n",
    "    image = tf.image.stateless_random_saturation(image, lower=1, upper=1.2, seed=seed)\n",
    "    image = tf.image.stateless_random_jpeg_quality(image, min_jpeg_quality=50, max_jpeg_quality=100, seed=seed)\n",
    "    #image = tf.image.stateless_random_crop(image, size=(60,60,3), seed=seed)\n",
    "    #image = tf.image.stateless_random_flip_left_right(image, seed)\n",
    "    #image = tf.image.stateless_random_flip_up_down(image, seed)\n",
    "    return image\n",
    "\n",
    "def preprocess(X_batch, y_batch):\n",
    "    X_batch = standardizer.transform(X_batch) # Standardize\n",
    "    return X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning up variables to prevent loading data multiple times (which may cause memory issue)\n",
    "try:\n",
    "   del train_dset\n",
    "   del val_dset\n",
    "   print('Clear previously loaded data.')\n",
    "except:\n",
    "   pass\n",
    "\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "IMG_SHAPE = (128, 128, 3)\n",
    "loader = ImageLabelLoader(class_names, img_shape=IMG_SHAPE)\n",
    "standardizer = Standardizer()\n",
    "normalization_samples_batch = ceil((num_train * 0.4) / BATCH_SIZE)\n",
    "\n",
    "# Force image load and preprocessing with specific device\n",
    "with tf.device(CPU):\n",
    "    # Train dataset input pipeline\n",
    "    train_dset = tf.data.Dataset.from_tensor_slices(train_paths)\n",
    "    train_dset = train_dset.map(loader.load, num_parallel_calls=tf.data.experimental.AUTOTUNE) # Load from path to image, label\n",
    "    train_dset = train_dset.map(wrap_augment, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    train_dset = train_dset.cache()\n",
    "    train_dset = train_dset.shuffle(buffer_size=num_train, reshuffle_each_iteration=True)\n",
    "    train_dset = train_dset.batch(BATCH_SIZE, drop_remainder=False)\n",
    "    standardizer.fit(train_dset, num_samples_batch=normalization_samples_batch) # Fit mean and std to train set\n",
    "    train_dset = train_dset.map(preprocess, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    train_dset = train_dset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    # Validation dataset input pipeline\n",
    "    val_dset = tf.data.Dataset.from_tensor_slices(val_paths)\n",
    "    val_dset = val_dset.map(loader.load, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    val_dset = val_dset.batch(BATCH_SIZE, drop_remainder=False)\n",
    "    val_dset = val_dset.map(preprocess, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    val_dset = val_dset.cache()\n",
    "    val_dset = val_dset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(tf.keras.Model):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ConvNet, self).__init__()\n",
    "\n",
    "        initializer = tf.initializers.VarianceScaling(scale=2.0)\n",
    "\n",
    "        self.conv_1 = tf.keras.layers.Conv2D(64, (3,3), strides=1, padding=\"same\", activation='relu',kernel_initializer=initializer)\n",
    "        self.batch_1 = tf.keras.layers.BatchNormalization() \n",
    "\n",
    "        self.maxpool_1 = tf.keras.layers.MaxPool2D() \n",
    "\n",
    "        self.conv_3 = tf.keras.layers.Conv2D(128, (1,1), strides=1, padding=\"same\", activation='relu', kernel_initializer=initializer)\n",
    "        self.batch_3 = tf.keras.layers.BatchNormalization()\n",
    "\n",
    "        self.maxpool_2 = tf.keras.layers.MaxPool2D() \n",
    "        \n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.dropout = tf.keras.layers.Dropout(rate=0.7)\n",
    "        self.fc = tf.keras.layers.Dense(num_classes, activation='softmax', kernel_initializer=initializer, kernel_regularizer=tf.keras.regularizers.l2())\n",
    "    \n",
    "    def call(self, x, training=False):\n",
    "        \n",
    "        x = self.conv_1(x)\n",
    "        x = self.batch_1(x, training=training)\n",
    "\n",
    "        x = self.maxpool_1(x)\n",
    "\n",
    "        x = self.conv_3(x)\n",
    "        x = self.batch_3(x, training=training)\n",
    "\n",
    "        x = self.maxpool_2(x)\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        x = self.dropout(x, training=training)\n",
    "        scores = self.fc(x)\n",
    "\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "decay_steps = 1\n",
    "decay_rate = 0.95\n",
    "\n",
    "model = ConvNet(num_classes)\n",
    "\n",
    "lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
    "                                    initial_learning_rate=lr,\n",
    "                                    decay_steps=decay_steps,\n",
    "                                    decay_rate=decay_rate,\n",
    "                                    staircase=False)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "\n",
    "solver = TensorflowSolver(model, optimizer, train_dset, val_dset,\n",
    "                            num_epochs=10,\n",
    "                            verbose=True)\n",
    "solver.train(device=GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_acc_history_epoch(solver.loss_history, solver.val_loss_history, solver.train_acc_history, solver.val_acc_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPTIMIZER = 'adam'\n",
    "LOSS = 'sparse_categorical_crossentropy'\n",
    "TENSORBOARD_DIR = 'tensorboard/logs/fit/'\n",
    "\n",
    "model = ConvNet(num_classes)\n",
    "model.compile(optimizer=OPTIMIZER,\n",
    "            loss=LOSS,\n",
    "            metrics=[tf.keras.metrics.sparse_categorical_accuracy])\n",
    "\n",
    "# Setup tensorboard\n",
    "log_dir = TENSORBOARD_DIR + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "with tf.device(GPU):\n",
    "    trainer = model.fit(train_dset, epochs=3, validation_data=val_dset, callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_acc_history_epoch(trainer.history['loss'], trainer.history['val_loss'], trainer.history['sparse_categorical_accuracy'], trainer.history['val_sparse_categorical_accuracy'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "name": "python369jvsc74a57bd031f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "metadata": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
